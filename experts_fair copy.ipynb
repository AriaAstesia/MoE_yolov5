{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('/data/ouyuan/yolov5')\n",
    "from tqdm import tqdm\n",
    "from utils.dataloaders import create_dataloader\n",
    "from moe import ModelSuffix, ModelName\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from utils.general import (\n",
    "    LOGGER,\n",
    "    TQDM_BAR_FORMAT,\n",
    "    Profile,\n",
    "    check_amp,\n",
    "    check_dataset,\n",
    "    check_file,\n",
    "    check_git_info,\n",
    "    check_git_status,\n",
    "    check_img_size,\n",
    "    check_requirements,\n",
    "    check_suffix,\n",
    "    check_yaml,\n",
    "    colorstr,\n",
    "    get_latest_run,\n",
    "    increment_path,\n",
    "    init_seeds,\n",
    "    intersect_dicts,\n",
    "    labels_to_class_weights,\n",
    "    labels_to_image_weights,\n",
    "    methods,\n",
    "    one_cycle,\n",
    "    print_args,\n",
    "    print_mutation,\n",
    "    strip_optimizer,\n",
    "    yaml_save,\n",
    "    increment_path,\n",
    "    non_max_suppression,\n",
    "    scale_boxes,\n",
    "    xywh2xyxy,\n",
    "    xyxy2xywh,\n",
    ")\n",
    "from utils.torch_utils import (\n",
    "    EarlyStopping,\n",
    "    ModelEMA,\n",
    "    de_parallel,\n",
    "    select_device,\n",
    "    smart_DDP,\n",
    "    smart_optimizer,\n",
    "    smart_resume,\n",
    "    torch_distributed_zero_first,\n",
    ")\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.metrics import ConfusionMatrix, ap_per_class, box_iou\n",
    "from utils.plots import output_to_target, plot_images, plot_val_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/ouyuan/datasets/cityscapes/train_320.cache... 320 images, 1 backgrounds, 0 corrupt: 100%|██████████| 320/320 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000000_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000022_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000030_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000031_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000042_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000060_000019_leftImg8bit.png: 15 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000061_000019_leftImg8bit.png: 34 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000062_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000064_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000078_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000079_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000080_000019_leftImg8bit.png: 22 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000090_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000092_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000096_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000098_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bochum/bochum_000000_000885_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bochum/bochum_000000_004748_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bochum/bochum_000000_007651_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bochum/bochum_000000_013209_leftImg8bit.png: 1 duplicate labels removed\n"
     ]
    }
   ],
   "source": [
    "LOCAL_RANK = int(os.getenv(\"LOCAL_RANK\", -1))\n",
    "RANK = int(os.getenv(\"RANK\", -1))\n",
    "WORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", 1))\n",
    "torch.backends.cudnn.enabled = False\n",
    "data = 'cityscapes320.yaml'\n",
    "imgsz = 640\n",
    "batch_size = 32\n",
    "gs = 32\n",
    "workers = 8\n",
    "label_smoothing = 0.0\n",
    "seed = 19260817\n",
    "init_seeds(seed + 1 + RANK, deterministic=True)\n",
    "single_cls = False\n",
    "epochs = 100\n",
    "nc = 80\n",
    "data = check_yaml(data)\n",
    "hyp = '../data/hyps/hyp.scratch-low.yaml'\n",
    "with open(hyp, errors=\"ignore\") as f:\n",
    "    hyp = yaml.safe_load(f)\n",
    "data_dict = check_dataset(data)\n",
    "train_path, val_path = data_dict[\"train\"], data_dict[\"val\"]\n",
    "train_loader, dataset = create_dataloader(\n",
    "        train_path,\n",
    "        imgsz,\n",
    "        batch_size,\n",
    "        gs,\n",
    "        single_cls,\n",
    "        hyp=hyp,\n",
    "        rank=LOCAL_RANK,\n",
    "        prefix=colorstr(\"train: \"),\n",
    "        shuffle=True,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding expert yolov5n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding expert yolov5s\n",
      "Adding expert yolov5m\n",
      "Adding expert yolov5l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 101 weight(decay=0.0), 104 weight(decay=0.0005), 104 bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding expert yolov5x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 75 weight(decay=0.0), 79 weight(decay=0.0005), 79 bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding expert yolov5n6\n",
      "Adding expert yolov5s6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 75 weight(decay=0.0), 79 weight(decay=0.0005), 79 bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 103 weight(decay=0.0), 107 weight(decay=0.0005), 107 bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding expert yolov5m6\n",
      "Adding expert yolov5l6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 131 weight(decay=0.0), 135 weight(decay=0.0005), 135 bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding expert yolov5x6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 159 weight(decay=0.0), 163 weight(decay=0.0005), 163 bias\n"
     ]
    }
   ],
   "source": [
    "experts_list = []\n",
    "optimizer= []\n",
    "optimizer_type = 'Adam'\n",
    "for i in range(len(ModelSuffix)):\n",
    "        model_name = str.replace(ModelName, 'tmp', ModelSuffix[i])\n",
    "        #model = Model(os.path.join('../models', model_name + '.yaml'), ch=3, nc=80)\n",
    "        print(f'Adding expert {model_name}')\n",
    "        model = torch.load(os.path.join('..', model_name + '.pt'), map_location=\"cpu\")[\"model\"].float()\n",
    "        experts_list.append(model)\n",
    "        freeze = 10 if i < 5 else 12\n",
    "        freeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze\n",
    "        for k, v in experts_list[i].named_parameters():\n",
    "                v.requires_grad = True  # train all layers\n",
    "                if any(x in k for x in freeze):\n",
    "                        # print(f'freezing {model_name}\\'s {k}')\n",
    "                        v.requires_grad = False\n",
    "                \n",
    "        optimizer.append(smart_optimizer(experts_list[i], optimizer_type, hyp[\"lr0\"], hyp[\"momentum\"], hyp[\"weight_decay\"]))\n",
    "        experts_list[i].to(device)\n",
    "        \n",
    "nl = de_parallel(experts_list[0]).model[-1].nl  # number of detection layers (to scale hyps)\n",
    "hyp[\"box\"] *= 3 / nl  # scale to layers\n",
    "hyp[\"cls\"] *= nc / 80 * 3 / nl  # scale to classes and layers\n",
    "hyp[\"obj\"] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\n",
    "hyp[\"label_smoothing\"] = label_smoothing\n",
    "compute_loss1 = ComputeLoss(experts_list[0])\n",
    "\n",
    "if len(experts_list) > 5:\n",
    "        compute_loss2 = ComputeLoss(experts_list[5])\n",
    "        \n",
    "for i in range(len(experts_list)):\n",
    "        experts_list[i].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thres = 0.001\n",
    "iou_thres = 0.6\n",
    "max_det = 300\n",
    "iouv = torch.linspace(0.5, 0.95, 10, device=device)  # iou vector for mAP@0.5:0.95\n",
    "niou = iouv.numel()\n",
    "seen = 0\n",
    "plots = True\n",
    "save_dir = Path('./exp_tmp')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "names = experts_list[0].names if hasattr(experts_list[0], \"names\") else experts_list[0].module.names  # get class names\n",
    "if isinstance(names, (list, tuple)):  # old format\n",
    "    names = dict(enumerate(names))\n",
    "task = 'val'\n",
    "verbose = False\n",
    "training = False\n",
    "\n",
    "dt = Profile(device=device), Profile(device=device), Profile(device=device)  # profiling times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  expert_id       Size\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      99/99      2.68G    0.03837    0.02341    0.00346          0        640: 100%|██████████| 10/10 [07:54<00:00, 47.45s/it]\n"
     ]
    }
   ],
   "source": [
    "experts = []\n",
    "experts_id = []\n",
    "pbar = enumerate(train_loader)\n",
    "LOGGER.info((\"\\n\" + \"%11s\" * 7) % (\"Epoch\", \"GPU_mem\", \"box_loss\", \"obj_loss\", \"cls_loss\", \"expert_id\", \"Size\"))\n",
    "if RANK in {-1, 0}:\n",
    "    pbar = tqdm(pbar, total=len(train_loader), bar_format=TQDM_BAR_FORMAT)  # progress bar\n",
    "\n",
    "mloss = torch.zeros(10, 3, device=device)  # mean losses\n",
    "cnt = torch.zeros(10, device=device)  # mean losses\n",
    "\n",
    "for batch_i, (imgs, targets, paths, _) in pbar:  # window -------------------------------------------------------------\n",
    "    id = 0\n",
    "    experts_id.append(id)\n",
    "    experts.append(copy.deepcopy(experts_list[id]).cpu())\n",
    "    torch.cuda.empty_cache()\n",
    "    experts_list[id].to(device)\n",
    "    experts_list[id].train()\n",
    "    \n",
    "    if batch_i == len(train_loader) - 1:\n",
    "        continue\n",
    "    \n",
    "    imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    optimizer[id].zero_grad()\n",
    "    for epoch in range(epochs):  # retrain ------------------------------------------------------------------\n",
    "        # Update mosaic border (optional)\n",
    "        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
    "        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
    "\n",
    "        \n",
    "        # if RANK != -1:\n",
    "        #     train_loader.sampler.set_epoch(epoch)\n",
    "        \n",
    "        # Forward\n",
    "        #with torch.cuda.amp.autocast(amp):\n",
    "        pred = experts_list[id](imgs)  # forward\n",
    "        # print(len(pred))\n",
    "        if id < 5:\n",
    "            loss, loss_items = compute_loss1(pred, targets)  # loss scaled by batch_size\n",
    "        else:\n",
    "            loss, loss_items = compute_loss2(pred, targets)  # loss scaled by batch_size\n",
    "            \n",
    "        if RANK != -1:\n",
    "            loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\n",
    "        \n",
    "        if torch.any(torch.isnan(loss_items)):\n",
    "            torch.cuda.empty_cache()\n",
    "            nb, _, height, width = imgs.shape\n",
    "            experts_list[id].eval()\n",
    "            preds, train_out = experts_list[id](imgs)\n",
    "            preds = preds.detach()\n",
    "            targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
    "            lb = [] # for autolabelling\n",
    "            preds = non_max_suppression(\n",
    "                preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\n",
    "            )\n",
    "            plot_images(imgs, targets, paths, save_dir / f\"val_batch{batch_i}_id{id}_labels.jpg\", names)  # labels\n",
    "            plot_images(imgs, output_to_target(preds), paths, save_dir / f\"val_batch{batch_i}_id{id}_pred.jpg\", names)  # pred\n",
    "            assert False\n",
    "            \n",
    "        \n",
    "        #print(loss, latency)\n",
    "        # Backward\n",
    "        #scaler.scale(loss).backward()\n",
    "        optimizer[id].zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer[id].step()\n",
    "        # Log\n",
    "        if RANK in {-1, 0}:\n",
    "            mloss[id] = (mloss[id] * cnt[id] + loss_items) / (cnt[id] + 1)  # update mean losses\n",
    "            mem = f\"{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G\"  # (GB)\n",
    "            pbar.set_description(\n",
    "                (\"%11s\" * 2 + \"%11.4g\" * 5)\n",
    "                % (f\"{epoch}/{epochs - 1}\", mem, *mloss[id], id, imgs.shape[-1])\n",
    "            )\n",
    "            cnt[id] += 1\n",
    "        # end retrain ------------------------------------------------------------------------------------------------\n",
    "    experts_list[id].cpu()\n",
    "    imgs, targets = imgs.cpu(), targets.cpu()\n",
    "    # end window ----------------------------------------------------------------------------------------------------\n",
    "# end training -----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Saving experts to ./exp_3\n"
     ]
    }
   ],
   "source": [
    "print(len(experts))\n",
    "for i in range(len(experts_list)):\n",
    "    experts_list[i].cpu()\n",
    "torch.cuda.empty_cache()\n",
    "save_tmp = './exp_tmp'\n",
    "save_dir = './exp_1'\n",
    "cnt = 0\n",
    "while(1):\n",
    "    cnt += 1\n",
    "    save_dir = save_tmp.replace('tmp', f'{cnt}')\n",
    "    # print(f'Testing {save_dir}')\n",
    "    if not os.path.exists(save_dir):\n",
    "        print(f'Saving experts to {save_dir}')\n",
    "        os.makedirs(save_dir)\n",
    "        break\n",
    "\n",
    "for i, expert in enumerate(experts):\n",
    "    expert_dir = os.path.join(save_dir, 'fair')\n",
    "    if not os.path.exists(expert_dir):\n",
    "        os.makedirs(expert_dir)\n",
    "    torch.save(expert, os.path.join(expert_dir, 'expert_%d_id_%d.ckpt'%(i, experts_id[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(detections, labels, iouv):\n",
    "    \"\"\"\n",
    "    Return correct prediction matrix.\n",
    "\n",
    "    Arguments:\n",
    "        detections (array[N, 6]), x1, y1, x2, y2, conf, class\n",
    "        labels (array[M, 5]), class, x1, y1, x2, y2\n",
    "    Returns:\n",
    "        correct (array[N, 10]), for 10 IoU levels\n",
    "    \"\"\"\n",
    "    correct = np.zeros((detections.shape[0], iouv.shape[0])).astype(bool)\n",
    "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "    correct_class = labels[:, 0:1] == detections[:, 5]\n",
    "    for i in range(len(iouv)):\n",
    "        x = torch.where((iou >= iouv[i]) & correct_class)  # IoU > threshold and classes match\n",
    "        if x[0].shape[0]:\n",
    "            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]\n",
    "            if x[0].shape[0] > 1:\n",
    "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "                # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "            correct[matches[:, 1].astype(int), i] = True\n",
    "    return torch.tensor(correct, dtype=torch.bool, device=iouv.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.35it/s]\n",
      "                   all        320       5313      0.503      0.209      0.228      0.132\n",
      "Speed: 0.1ms pre-process, 15.2ms inference, 1.0ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mexp_3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jdict, stats, ap, ap_class = [], [], [], []\n",
    "save_dir = Path(save_dir)\n",
    "loss = torch.zeros(3, device=device)\n",
    "pbar = enumerate(train_loader)\n",
    "LOGGER.info((\"\\n\" + \"%11s\" * 7) % (\"Epoch\", \"GPU_mem\", \"box_loss\", \"obj_loss\", \"cls_loss\", \"Instances\", \"Size\"))\n",
    "if RANK in {-1, 0}:\n",
    "    pbar = tqdm(pbar, total=len(train_loader), bar_format=TQDM_BAR_FORMAT)  # progress bar\n",
    "    \n",
    "for batch_i, (im, targets, paths, shapes) in pbar:\n",
    "    experts[batch_i].to(device)\n",
    "    experts[batch_i].eval()\n",
    "    with dt[0]:\n",
    "        im = im.to(device, non_blocking=True)\n",
    "        targets = targets.to(device)\n",
    "        im = im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        bs, _, height, width = im.shape  # batch size, channels, height, width\n",
    "\n",
    "    with dt[1]:\n",
    "        preds, train_out = experts[batch_i](im)\n",
    "    preds = preds.detach()\n",
    "\n",
    "    id = experts_id[batch_i]\n",
    "    # Loss\n",
    "    if id < 5:\n",
    "        loss += compute_loss1(train_out, targets)[1]  # box, obj, cls\n",
    "    else:\n",
    "        loss += compute_loss2(train_out, targets)[1]  # box, obj, cls\n",
    "\n",
    "    # NMS\n",
    "    targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
    "    lb = [] # for autolabelling\n",
    "\n",
    "    with dt[2]:\n",
    "        preds = non_max_suppression(\n",
    "            preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\n",
    "        )\n",
    "\n",
    "    # Metrics\n",
    "    for si, pred in enumerate(preds):\n",
    "        labels = targets[targets[:, 0] == si, 1:]\n",
    "        nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions\n",
    "        path, shape = Path(paths[si]), shapes[si][0]\n",
    "        correct = torch.zeros(npr, niou, dtype=torch.bool, device=device)  # init\n",
    "        seen += 1\n",
    "\n",
    "        if npr == 0:\n",
    "            if nl:\n",
    "                stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))\n",
    "                if plots:\n",
    "                    confusion_matrix.process_batch(detections=None, labels=labels[:, 0])\n",
    "            continue\n",
    "\n",
    "        # Predictions\n",
    "        if single_cls:\n",
    "            pred[:, 5] = 0\n",
    "        predn = pred.clone()\n",
    "        scale_boxes(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n",
    "\n",
    "        # Evaluate\n",
    "        if nl:\n",
    "            tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n",
    "            scale_boxes(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n",
    "            labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n",
    "            correct = process_batch(predn, labelsn, iouv)\n",
    "            if plots:\n",
    "                confusion_matrix.process_batch(predn, labelsn)\n",
    "        stats.append((correct, pred[:, 4], pred[:, 5], labels[:, 0]))  # (correct, conf, pcls, tcls)\n",
    "\n",
    "\n",
    "    # Plot images\n",
    "    if plots and batch_i < 3:\n",
    "        plot_images(im, targets, paths, save_dir / f\"val_batch{batch_i}_labels.jpg\", names)  # labels\n",
    "        plot_images(im, output_to_target(preds), paths, save_dir / f\"val_batch{batch_i}_pred.jpg\", names)  # pred\n",
    "        \n",
    "    experts[batch_i].cpu()\n",
    "\n",
    "# Compute metrics\n",
    "stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]  # to numpy\n",
    "if len(stats) and stats[0].any():\n",
    "    tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class\n",
    "\n",
    "# Print results\n",
    "pf = \"%22s\" + \"%11i\" * 2 + \"%11.3g\" * 4  # print format\n",
    "LOGGER.info(pf % (\"all\", seen, nt.sum(), mp, mr, map50, map))\n",
    "if nt.sum() == 0:\n",
    "    LOGGER.warning(f\"WARNING ⚠️ no labels found in {task} set, can not compute metrics without labels\")\n",
    "\n",
    "# Print results per class\n",
    "if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
    "    for i, c in enumerate(ap_class):\n",
    "        LOGGER.info(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "# Print speeds\n",
    "t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n",
    "if not training:\n",
    "    shape = (batch_size, 3, imgsz, imgsz)\n",
    "    LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}\" % t)\n",
    "\n",
    "# Plots\n",
    "if plots:\n",
    "    confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "\n",
    "# Return results\n",
    "if not training:\n",
    "    s = \"\"\n",
    "    LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
    "maps = np.zeros(nc) + map\n",
    "for i, c in enumerate(ap_class):\n",
    "    maps[c] = ap[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5028589934647845, 0.20925345921626198, 0.22783073171827053, 0.13212560817618027, 0.06084638833999634, 0.05650714039802551, 0.015317119657993317) [    0.12145    0.084467     0.28237    0.038006     0.13213     0.17992     0.12271     0.09596     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213\n",
      "     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213\n",
      "     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213     0.13213\n",
      "     0.13213     0.13213] (0.13176500797271729, 15.158962458372116, 0.9706936776638031)\n"
     ]
    }
   ],
   "source": [
    "print((mp, mr, map50, map, *(loss.cpu() / len(train_loader)).tolist()), maps, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
