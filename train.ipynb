{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('/data/ouyuan/yolov5')\n",
    "from tqdm import tqdm\n",
    "from utils.dataloaders import create_dataloader\n",
    "from moe import MoE, ModelSuffix, ModelName\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from utils.general import (\n",
    "    LOGGER,\n",
    "    TQDM_BAR_FORMAT,\n",
    "    Profile,\n",
    "    check_amp,\n",
    "    check_dataset,\n",
    "    check_file,\n",
    "    check_git_info,\n",
    "    check_git_status,\n",
    "    check_img_size,\n",
    "    check_requirements,\n",
    "    check_suffix,\n",
    "    check_yaml,\n",
    "    colorstr,\n",
    "    get_latest_run,\n",
    "    increment_path,\n",
    "    init_seeds,\n",
    "    intersect_dicts,\n",
    "    labels_to_class_weights,\n",
    "    labels_to_image_weights,\n",
    "    methods,\n",
    "    one_cycle,\n",
    "    print_args,\n",
    "    print_mutation,\n",
    "    strip_optimizer,\n",
    "    yaml_save,\n",
    "    increment_path,\n",
    "    non_max_suppression,\n",
    "    scale_boxes,\n",
    "    xywh2xyxy,\n",
    "    xyxy2xywh,\n",
    ")\n",
    "from utils.torch_utils import (\n",
    "    EarlyStopping,\n",
    "    ModelEMA,\n",
    "    de_parallel,\n",
    "    select_device,\n",
    "    smart_DDP,\n",
    "    smart_optimizer,\n",
    "    smart_resume,\n",
    "    torch_distributed_zero_first,\n",
    ")\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.metrics import ConfusionMatrix, ap_per_class, box_iou\n",
    "from utils.plots import output_to_target, plot_images, plot_val_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/ouyuan/datasets/cityscapes/train.cache... 2975 images, 7 backgrounds, 0 corrupt: 100%|██████████| 2975/2975 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000000_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000022_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000030_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000031_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000042_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000054_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000060_000019_leftImg8bit.png: 15 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000061_000019_leftImg8bit.png: 34 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000062_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000064_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000078_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000079_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000080_000019_leftImg8bit.png: 22 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000090_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000092_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000096_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/aachen/aachen_000098_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bochum/bochum_000000_000885_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bochum/bochum_000000_004748_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bochum/bochum_000000_007651_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bochum/bochum_000000_013209_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bremen/bremen_000060_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/bremen/bremen_000241_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000006_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000007_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000009_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000010_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000012_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000018_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000053_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000090_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000092_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/cologne/cologne_000100_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/darmstadt/darmstadt_000004_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/darmstadt/darmstadt_000021_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/darmstadt/darmstadt_000046_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/darmstadt/darmstadt_000061_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/darmstadt/darmstadt_000083_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hamburg/hamburg_000000_019892_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hamburg/hamburg_000000_032906_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hamburg/hamburg_000000_033506_leftImg8bit.png: 5 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hamburg/hamburg_000000_039420_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hamburg/hamburg_000000_053886_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hamburg/hamburg_000000_080438_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_014537_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_015587_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_018800_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_019456_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_022645_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_026356_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_034015_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_034141_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_047870_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/hanover/hanover_000000_051536_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/krefeld/krefeld_000000_004447_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/krefeld/krefeld_000000_013766_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/strasbourg/strasbourg_000000_014503_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/strasbourg/strasbourg_000001_030539_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/strasbourg/strasbourg_000001_030997_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/strasbourg/strasbourg_000001_051317_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/strasbourg/strasbourg_000001_052050_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/strasbourg/strasbourg_000001_058373_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/strasbourg/strasbourg_000001_064224_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/strasbourg/strasbourg_000001_064393_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/stuttgart/stuttgart_000109_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/stuttgart/stuttgart_000120_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/stuttgart/stuttgart_000168_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/stuttgart/stuttgart_000175_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/stuttgart/stuttgart_000184_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/tubingen/tubingen_000089_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000004_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000005_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000006_000019_leftImg8bit.png: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000011_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000012_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000013_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000017_000019_leftImg8bit.png: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000024_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000025_000019_leftImg8bit.png: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000037_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000056_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000058_000019_leftImg8bit.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ouyuan/datasets/cityscapes/images/train/ulm/ulm_000084_000019_leftImg8bit.png: 1 duplicate labels removed\n"
     ]
    }
   ],
   "source": [
    "LOCAL_RANK = int(os.getenv(\"LOCAL_RANK\", -1))\n",
    "RANK = int(os.getenv(\"RANK\", -1))\n",
    "WORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", 1))\n",
    "torch.backends.cudnn.enabled = False\n",
    "data = 'cityscapes.yaml'\n",
    "imgsz = 640\n",
    "batch_size = 32\n",
    "gs = 32\n",
    "workers = 8\n",
    "label_smoothing = 0.0\n",
    "seed = 19260817\n",
    "init_seeds(seed + 1 + RANK, deterministic=True)\n",
    "single_cls = False\n",
    "epochs = 100\n",
    "optimizer = 'Adam'\n",
    "nc = 80\n",
    "data = check_yaml(data)\n",
    "hyp = '../data/hyps/hyp.scratch-low.yaml'\n",
    "with open(hyp, errors=\"ignore\") as f:\n",
    "    hyp = yaml.safe_load(f)\n",
    "data_dict = check_dataset(data)\n",
    "train_path, val_path = data_dict[\"train\"], data_dict[\"val\"]\n",
    "train_loader, dataset = create_dataloader(\n",
    "        train_path,\n",
    "        imgsz,\n",
    "        batch_size,\n",
    "        gs,\n",
    "        single_cls,\n",
    "        hyp=hyp,\n",
    "        rank=LOCAL_RANK,\n",
    "        prefix=colorstr(\"train: \"),\n",
    "        shuffle=True,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "moe = MoE()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nl = de_parallel(moe.experts[0]).model[-1].nl  # number of detection layers (to scale hyps)\n",
    "hyp[\"box\"] *= 3 / nl  # scale to layers\n",
    "hyp[\"cls\"] *= nc / 80 * 3 / nl  # scale to classes and layers\n",
    "hyp[\"obj\"] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\n",
    "hyp[\"label_smoothing\"] = label_smoothing\n",
    "\n",
    "moe.to(device)\n",
    "\n",
    "compute_loss1 = ComputeLoss(moe.experts[0])\n",
    "compute_loss2 = ComputeLoss(moe.experts[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" for i in range(moe.experts_num):\n",
    "    for name, param in moe.experts[i].named_parameters():\n",
    "        param.requires_grad = False \"\"\"\n",
    "\n",
    "# layer 0 - 9 for 640; 0 - 11 for 1280\n",
    "\n",
    "for i in range(10):\n",
    "    freeze = 10 if i < 5 else 12\n",
    "    freeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze\n",
    "    for k, v in moe.experts[i].named_parameters():\n",
    "        v.requires_grad = True  # train all layers\n",
    "        if any(x in k for x in freeze):\n",
    "            #print(f'freezing {k}')\n",
    "            v.requires_grad = False\n",
    "\n",
    "for name, param in moe.latency_predictor.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# for name, param in moe.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 964 weight(decay=0.0), 1006 weight(decay=0.0005), 1006 bias\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = 'Adam'\n",
    "optimizer = smart_optimizer(moe, optimizer, hyp[\"lr0\"], hyp[\"momentum\"], hyp[\"weight_decay\"])\n",
    "#optimizer.add_param_group({'params': moe.w_gate, 'weight_decay': hyp[\"weight_decay\"]})\n",
    "# optimizer = torch.optim.Adam([{'params': moe.w_gate}], lr=hyp[\"lr0\"], betas=(hyp[\"momentum\"], 0.999))\n",
    "\n",
    "nb = len(train_loader)\n",
    "#amp = check_amp(moe)\n",
    "\n",
    "#scaler = torch.cuda.amp.GradScaler(enabled=amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './exp_tmp'\n",
    "conf_thres = 0.001\n",
    "iou_thres = 0.6\n",
    "max_det = 300\n",
    "iouv = torch.linspace(0.5, 0.95, 10, device=device)  # iou vector for mAP@0.5:0.95\n",
    "niou = iouv.numel()\n",
    "seen = 0\n",
    "plots = True\n",
    "save_dir = Path(save_dir)\n",
    "confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "names = moe.experts[0].names if hasattr(moe.experts[0], \"names\") else moe.experts[0].module.names  # get class names\n",
    "if isinstance(names, (list, tuple)):  # old format\n",
    "    names = dict(enumerate(names))\n",
    "task = 'val'\n",
    "verbose = False\n",
    "training = False\n",
    "\n",
    "dt = Profile(device=device), Profile(device=device), Profile(device=device)  # profiling times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([660, 6])\n",
      "cpu\n",
      "tensor([0.00000, 2.00000, 0.22496, 0.51735, 0.29484, 0.16851], device='cuda:0')\n",
      "tensor([0.00000, 2.00000, 0.39160, 0.45801, 0.01074, 0.00684])\n"
     ]
    }
   ],
   "source": [
    "teacher = torch.load(os.path.join('..', 'yolov5x.pt'), map_location=\"cpu\")[\"model\"].float()\n",
    "teacher.to(device)\n",
    "teacher.eval()\n",
    "teacher_conf_thres = 0.15\n",
    "teacher_iou_thres = 0.6\n",
    "for batch_i, (imgs, targets, paths, _) in enumerate(train_loader):\n",
    "    imgs = imgs.to(device, non_blocking=True).float() / 255\n",
    "    bs, _, height, width = imgs.shape\n",
    "    preds, train_out = teacher(imgs)\n",
    "    preds = preds.detach()\n",
    "    # targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
    "    lb = [] # for autolabelling\n",
    "    preds = non_max_suppression(\n",
    "        preds, teacher_conf_thres, teacher_iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\n",
    "    )\n",
    "    preds = torch.Tensor(output_to_target(preds)).cuda()\n",
    "    preds = preds[:,:-1]\n",
    "    preds[:, 2:] /= torch.tensor((width, height, width, height), device=device)\n",
    "    print(preds.shape)\n",
    "    print(targets.device)\n",
    "    print(preds[0])    \n",
    "    print(targets[0])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  expert_id       Size\n",
      "      99/99      12.5G   0.001412   0.002376  0.0006589          7        640:  11%|█         | 10/93 [11:49<1:38:06, 70.92s/it]\n"
     ]
    }
   ],
   "source": [
    "experts_moe = []\n",
    "experts_moe_id = []\n",
    "pbar = enumerate(train_loader)\n",
    "teacher.eval()\n",
    "LOGGER.info((\"\\n\" + \"%11s\" * 7) % (\"Epoch\", \"GPU_mem\", \"box_loss\", \"obj_loss\", \"cls_loss\", \"expert_id\", \"Size\"))\n",
    "if RANK in {-1, 0}:\n",
    "    pbar = tqdm(pbar, total=nb, bar_format=TQDM_BAR_FORMAT)  # progress bar\n",
    "for batch_i, (imgs, targets, paths, _) in pbar:  # window -------------------------------------------------------------\n",
    "    if batch_i == 0:\n",
    "        id = random.randint(0, 9)\n",
    "        experts_moe_id.append(id)\n",
    "        experts_moe.append(copy.deepcopy(moe.experts[id]).cpu())\n",
    "    if batch_i == 10:\n",
    "        break\n",
    "    if batch_i == len(train_loader) - 1:\n",
    "        continue\n",
    "    torch.cuda.empty_cache()\n",
    "    imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "    bs, _, height, width = imgs.shape\n",
    "    labels, train_out = teacher(imgs)\n",
    "    labels = labels.detach()\n",
    "    # targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
    "    lb = [] # for autolabelling\n",
    "    labels = non_max_suppression(\n",
    "        labels, teacher_conf_thres, teacher_iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\n",
    "    )\n",
    "    labels = torch.Tensor(output_to_target(labels)).cuda()\n",
    "    labels = labels[:,:-1]\n",
    "    labels[:, 2:] /= torch.tensor((width, height, width, height), device=device)\n",
    "    # targets = targets.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(epochs):  # retrain ------------------------------------------------------------------\n",
    "        moe.train()\n",
    "\n",
    "        # Update mosaic border (optional)\n",
    "        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
    "        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
    "\n",
    "        mloss = torch.zeros(3, device=device)  # mean losses\n",
    "        # if RANK != -1:\n",
    "        #     train_loader.sampler.set_epoch(epoch)\n",
    "        \n",
    "        # Forward\n",
    "        #with torch.cuda.amp.autocast(amp):\n",
    "        pred, id, latency = moe(imgs)  # forward\n",
    "        # print(len(pred))\n",
    "        if id < 5:\n",
    "            loss, loss_items = compute_loss1(pred, labels)  # loss scaled by batch_size\n",
    "        else:\n",
    "            loss, loss_items = compute_loss2(pred, labels)  # loss scaled by batch_size\n",
    "            \n",
    "        if RANK != -1:\n",
    "            loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\n",
    "        \n",
    "        if torch.any(torch.isnan(loss_items)):\n",
    "            torch.cuda.empty_cache()\n",
    "            bs, _, height, width = imgs.shape\n",
    "            moe.experts[id].eval()\n",
    "            preds, train_out = moe.experts[id](imgs)\n",
    "            preds = preds.detach()\n",
    "            targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
    "            lb = [] # for autolabelling\n",
    "            preds = non_max_suppression(\n",
    "                preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\n",
    "            )\n",
    "            plot_images(imgs, targets, paths, save_dir / f\"val_batch{batch_i}_id{id}_labels.jpg\", names)  # labels\n",
    "            plot_images(imgs, output_to_target(preds), paths, save_dir / f\"val_batch{batch_i}_id{id}_pred.jpg\", names)  # pred\n",
    "            assert False\n",
    "        \n",
    "        #print(loss, latency)\n",
    "        loss = loss + latency\n",
    "        # Backward\n",
    "        #scaler.scale(loss).backward()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log\n",
    "        if RANK in {-1, 0}:\n",
    "            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "            mem = f\"{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G\"  # (GB)\n",
    "            pbar.set_description(\n",
    "                (\"%11s\" * 2 + \"%11.4g\" * 5)\n",
    "                % (f\"{epoch}/{epochs - 1}\", mem, *mloss, id, imgs.shape[-1])\n",
    "            )\n",
    "        # end retrain ------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    moe.eval()\n",
    "    id = moe(imgs, switch=True)\n",
    "    experts_moe_id.append(id.cpu().detach())\n",
    "    experts_moe.append(copy.deepcopy(moe.experts[id]).cpu())\n",
    "    imgs, labels = imgs.cpu(), labels.cpu()\n",
    "\n",
    "    # end window ----------------------------------------------------------------------------------------------------\n",
    "# end training -----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Saving experts to ./exp_2\n"
     ]
    }
   ],
   "source": [
    "print(len(experts_moe))\n",
    "moe.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "save_tmp = './exp_tmp'\n",
    "save_dir = './exp_1'\n",
    "cnt = 0\n",
    "while(1):\n",
    "    cnt += 1\n",
    "    save_dir = save_tmp.replace('tmp', f'{cnt}')\n",
    "    # print(f'Testing {save_dir}')\n",
    "    if not os.path.exists(save_dir):\n",
    "        print(f'Saving experts to {save_dir}')\n",
    "        os.makedirs(save_dir)\n",
    "        break\n",
    "\n",
    "expert_dir = os.path.join(save_dir, 'moe')\n",
    "for i, expert in enumerate(experts_moe):\n",
    "    if not os.path.exists(expert_dir):\n",
    "        os.makedirs(expert_dir)\n",
    "    torch.save(expert, os.path.join(expert_dir, 'expert_%d_id_%d.ckpt'%(i, experts_moe_id[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate = torch.zeros((16, 8)).sum(dim=0)\n",
    "gate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(detections, labels, iouv):\n",
    "    \"\"\"\n",
    "    Return correct prediction matrix.\n",
    "\n",
    "    Arguments:\n",
    "        detections (array[N, 6]), x1, y1, x2, y2, conf, class\n",
    "        labels (array[M, 5]), class, x1, y1, x2, y2\n",
    "    Returns:\n",
    "        correct (array[N, 10]), for 10 IoU levels\n",
    "    \"\"\"\n",
    "    correct = np.zeros((detections.shape[0], iouv.shape[0])).astype(bool)\n",
    "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "    correct_class = labels[:, 0:1] == detections[:, 5]\n",
    "    for i in range(len(iouv)):\n",
    "        x = torch.where((iou >= iouv[i]) & correct_class)  # IoU > threshold and classes match\n",
    "        if x[0].shape[0]:\n",
    "            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]\n",
    "            if x[0].shape[0] > 1:\n",
    "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "                # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "            correct[matches[:, 1].astype(int), i] = True\n",
    "    return torch.tensor(correct, dtype=torch.bool, device=iouv.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' jdict, stats, ap, ap_class = [], [], [], []\\nsave_dir = Path(save_dir)\\nloss = torch.zeros(3, device=device)\\npbar = enumerate(train_loader)\\nLOGGER.info((\"\\n\" + \"%11s\" * 7) % (\"Epoch\", \"GPU_mem\", \"box_loss\", \"obj_loss\", \"cls_loss\", \"Instances\", \"Size\"))\\nif RANK in {-1, 0}:\\n    pbar = tqdm(pbar, total=nb, bar_format=TQDM_BAR_FORMAT)  # progress bar\\n    \\nfor batch_i, (im, targets, paths, shapes) in pbar:\\n    experts_moe[batch_i].to(device)\\n    experts_moe[batch_i].eval()\\n    with dt[0]:\\n        im = im.to(device, non_blocking=True)\\n        targets = targets.to(device)\\n        im = im.float()  # uint8 to fp16/32\\n        im /= 255  # 0 - 255 to 0.0 - 1.0\\n        bs, _, height, width = im.shape  # batch size, channels, height, width\\n\\n    with dt[1]:\\n        preds, train_out = experts_moe[batch_i](im)\\n    preds = preds.detach()\\n\\n    id = experts_moe_id[batch_i]\\n    # Loss\\n    if id < 5:\\n        loss += compute_loss1(train_out, targets)[1]  # box, obj, cls\\n    else:\\n        loss += compute_loss2(train_out, targets)[1]  # box, obj, cls\\n\\n    # NMS\\n    targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\\n    lb = [] # for autolabelling\\n\\n    with dt[2]:\\n        preds = non_max_suppression(\\n            preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\\n        )\\n\\n    # Metrics\\n    for si, pred in enumerate(preds):\\n        labels = targets[targets[:, 0] == si, 1:]\\n        nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions\\n        path, shape = Path(paths[si]), shapes[si][0]\\n        correct = torch.zeros(npr, niou, dtype=torch.bool, device=device)  # init\\n        seen += 1\\n\\n        if npr == 0:\\n            if nl:\\n                stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))\\n                if plots:\\n                    confusion_matrix.process_batch(detections=None, labels=labels[:, 0])\\n            continue\\n\\n        # Predictions\\n        if single_cls:\\n            pred[:, 5] = 0\\n        predn = pred.clone()\\n        scale_boxes(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\\n\\n        # Evaluate\\n        if nl:\\n            tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\\n            scale_boxes(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\\n            labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\\n            correct = process_batch(predn, labelsn, iouv)\\n            if plots:\\n                confusion_matrix.process_batch(predn, labelsn)\\n        stats.append((correct, pred[:, 4], pred[:, 5], labels[:, 0]))  # (correct, conf, pcls, tcls)\\n\\n\\n    # Plot images\\n    if plots and batch_i < 3:\\n        plot_images(im, targets, paths, save_dir / f\"val_batch{batch_i}_labels.jpg\", names)  # labels\\n        plot_images(im, output_to_target(preds), paths, save_dir / f\"val_batch{batch_i}_pred.jpg\", names)  # pred\\n        \\n    experts_moe[batch_i].cpu()\\n\\n# Compute metrics\\nstats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]  # to numpy\\nif len(stats) and stats[0].any():\\n    tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\\n    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\\n    mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\\nnt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class\\n\\n# Print results\\npf = \"%22s\" + \"%11i\" * 2 + \"%11.3g\" * 4  # print format\\nLOGGER.info(pf % (\"all\", seen, nt.sum(), mp, mr, map50, map))\\nif nt.sum() == 0:\\n    LOGGER.warning(f\"WARNING ⚠️ no labels found in {task} set, can not compute metrics without labels\")\\n\\n# Print results per class\\nif (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\\n    for i, c in enumerate(ap_class):\\n        LOGGER.info(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\\n\\n# Print speeds\\nt = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\\nif not training:\\n    shape = (batch_size, 3, imgsz, imgsz)\\n    LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}\" % t)\\n\\n# Plots\\nif plots:\\n    confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\\n\\n# Return results\\nif not training:\\n    s = \"\"\\n    LOGGER.info(f\"Results saved to {colorstr(\\'bold\\', save_dir)}{s}\")\\nmaps = np.zeros(nc) + map\\nfor i, c in enumerate(ap_class):\\n    maps[c] = ap[i]\\n    \\nprint((mp, mr, map50, map, *(loss.cpu() / len(train_loader)).tolist()), maps, t) '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" jdict, stats, ap, ap_class = [], [], [], []\n",
    "save_dir = Path(save_dir)\n",
    "loss = torch.zeros(3, device=device)\n",
    "pbar = enumerate(train_loader)\n",
    "LOGGER.info((\"\\n\" + \"%11s\" * 7) % (\"Epoch\", \"GPU_mem\", \"box_loss\", \"obj_loss\", \"cls_loss\", \"Instances\", \"Size\"))\n",
    "if RANK in {-1, 0}:\n",
    "    pbar = tqdm(pbar, total=nb, bar_format=TQDM_BAR_FORMAT)  # progress bar\n",
    "    \n",
    "for batch_i, (im, targets, paths, shapes) in pbar:\n",
    "    experts_moe[batch_i].to(device)\n",
    "    experts_moe[batch_i].eval()\n",
    "    with dt[0]:\n",
    "        im = im.to(device, non_blocking=True)\n",
    "        targets = targets.to(device)\n",
    "        im = im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        bs, _, height, width = im.shape  # batch size, channels, height, width\n",
    "\n",
    "    with dt[1]:\n",
    "        preds, train_out = experts_moe[batch_i](im)\n",
    "    preds = preds.detach()\n",
    "\n",
    "    id = experts_moe_id[batch_i]\n",
    "    # Loss\n",
    "    if id < 5:\n",
    "        loss += compute_loss1(train_out, targets)[1]  # box, obj, cls\n",
    "    else:\n",
    "        loss += compute_loss2(train_out, targets)[1]  # box, obj, cls\n",
    "\n",
    "    # NMS\n",
    "    targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
    "    lb = [] # for autolabelling\n",
    "\n",
    "    with dt[2]:\n",
    "        preds = non_max_suppression(\n",
    "            preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\n",
    "        )\n",
    "\n",
    "    # Metrics\n",
    "    for si, pred in enumerate(preds):\n",
    "        labels = targets[targets[:, 0] == si, 1:]\n",
    "        nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions\n",
    "        path, shape = Path(paths[si]), shapes[si][0]\n",
    "        correct = torch.zeros(npr, niou, dtype=torch.bool, device=device)  # init\n",
    "        seen += 1\n",
    "\n",
    "        if npr == 0:\n",
    "            if nl:\n",
    "                stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))\n",
    "                if plots:\n",
    "                    confusion_matrix.process_batch(detections=None, labels=labels[:, 0])\n",
    "            continue\n",
    "\n",
    "        # Predictions\n",
    "        if single_cls:\n",
    "            pred[:, 5] = 0\n",
    "        predn = pred.clone()\n",
    "        scale_boxes(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n",
    "\n",
    "        # Evaluate\n",
    "        if nl:\n",
    "            tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n",
    "            scale_boxes(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n",
    "            labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n",
    "            correct = process_batch(predn, labelsn, iouv)\n",
    "            if plots:\n",
    "                confusion_matrix.process_batch(predn, labelsn)\n",
    "        stats.append((correct, pred[:, 4], pred[:, 5], labels[:, 0]))  # (correct, conf, pcls, tcls)\n",
    "\n",
    "\n",
    "    # Plot images\n",
    "    if plots and batch_i < 3:\n",
    "        plot_images(im, targets, paths, save_dir / f\"val_batch{batch_i}_labels.jpg\", names)  # labels\n",
    "        plot_images(im, output_to_target(preds), paths, save_dir / f\"val_batch{batch_i}_pred.jpg\", names)  # pred\n",
    "        \n",
    "    experts_moe[batch_i].cpu()\n",
    "\n",
    "# Compute metrics\n",
    "stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]  # to numpy\n",
    "if len(stats) and stats[0].any():\n",
    "    tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class\n",
    "\n",
    "# Print results\n",
    "pf = \"%22s\" + \"%11i\" * 2 + \"%11.3g\" * 4  # print format\n",
    "LOGGER.info(pf % (\"all\", seen, nt.sum(), mp, mr, map50, map))\n",
    "if nt.sum() == 0:\n",
    "    LOGGER.warning(f\"WARNING ⚠️ no labels found in {task} set, can not compute metrics without labels\")\n",
    "\n",
    "# Print results per class\n",
    "if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
    "    for i, c in enumerate(ap_class):\n",
    "        LOGGER.info(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "# Print speeds\n",
    "t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n",
    "if not training:\n",
    "    shape = (batch_size, 3, imgsz, imgsz)\n",
    "    LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}\" % t)\n",
    "\n",
    "# Plots\n",
    "if plots:\n",
    "    confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "\n",
    "# Return results\n",
    "if not training:\n",
    "    s = \"\"\n",
    "    LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
    "maps = np.zeros(nc) + map\n",
    "for i, c in enumerate(ap_class):\n",
    "    maps[c] = ap[i]\n",
    "    \n",
    "print((mp, mr, map50, map, *(loss.cpu() / len(train_loader)).tolist()), maps, t) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = glob.glob(os.path.join(expert_dir, f'expert_{0}_id_*.ckpt'))[0]\n",
    "id = int(model_name.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "# model = torch.load(os.path.join('..', 'yolov5x' + '.pt'), map_location=\"cpu\")[\"model\"].float()\n",
    "# print(model.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      " 11%|█         | 10/93 [00:17<02:21,  1.70s/it]\n",
      "                   all        320       6239      0.512      0.164      0.172      0.105\n",
      "Speed: 0.1ms pre-process, 10.7ms inference, 0.8ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mexp_2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jdict, stats, ap, ap_class = [], [], [], []\n",
    "save_dir = Path(save_dir)\n",
    "loss = torch.zeros(3, device=device)\n",
    "pbar = enumerate(train_loader)\n",
    "LOGGER.info((\"\\n\" + \"%11s\" * 7) % (\"Epoch\", \"GPU_mem\", \"box_loss\", \"obj_loss\", \"cls_loss\", \"Instances\", \"Size\"))\n",
    "if RANK in {-1, 0}:\n",
    "    pbar = tqdm(pbar, total=nb, bar_format=TQDM_BAR_FORMAT)  # progress bar\n",
    "    \n",
    "for batch_i, (im, targets, paths, shapes) in pbar:\n",
    "    torch.cuda.empty_cache()\n",
    "    if batch_i == 10:\n",
    "        break\n",
    "    model_name = glob.glob(os.path.join(expert_dir, f'expert_{batch_i}_id_*.ckpt'))[0]\n",
    "    model = torch.load(model_name, map_location=\"cpu\").float()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with dt[0]:\n",
    "        im = im.to(device, non_blocking=True)\n",
    "        targets = targets.to(device)\n",
    "        im = im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        bs, _, height, width = im.shape  # batch size, channels, height, width\n",
    "\n",
    "    \n",
    "    teacher_labels, train_out = teacher(im)\n",
    "    teacher_labels = teacher_labels.detach()\n",
    "    # targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
    "    lb = [] # for autolabelling\n",
    "    teacher_labels = non_max_suppression(\n",
    "        teacher_labels, teacher_conf_thres, teacher_iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\n",
    "    )\n",
    "    teacher_labels = torch.Tensor(output_to_target(teacher_labels)).cuda()\n",
    "    teacher_labels = teacher_labels[:,:-1]\n",
    "    teacher_labels[:, 2:] /= torch.tensor((width, height, width, height), device=device)\n",
    "    \n",
    "    with dt[1]:\n",
    "        preds, train_out = model(im)\n",
    "    preds = preds.detach()\n",
    "\n",
    "    id = int(model_name.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "    # Loss\n",
    "    if id < 5:\n",
    "        loss += compute_loss1(train_out, teacher_labels)[1]  # box, obj, cls\n",
    "    else:\n",
    "        loss += compute_loss2(train_out, teacher_labels)[1]  # box, obj, cls\n",
    "\n",
    "    # NMS\n",
    "    teacher_labels[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
    "    lb = [] # for autolabelling\n",
    "\n",
    "    with dt[2]:\n",
    "        preds = non_max_suppression(\n",
    "            preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det\n",
    "        )\n",
    "\n",
    "    # Metrics\n",
    "    for si, pred in enumerate(preds):\n",
    "        labels = teacher_labels[teacher_labels[:, 0] == si, 1:]\n",
    "        nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions\n",
    "        path, shape = Path(paths[si]), shapes[si][0]\n",
    "        correct = torch.zeros(npr, niou, dtype=torch.bool, device=device)  # init\n",
    "        seen += 1\n",
    "\n",
    "        if npr == 0:\n",
    "            if nl:\n",
    "                stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))\n",
    "                if plots:\n",
    "                    confusion_matrix.process_batch(detections=None, labels=labels[:, 0])\n",
    "            continue\n",
    "\n",
    "        # Predictions\n",
    "        if single_cls:\n",
    "            pred[:, 5] = 0\n",
    "        predn = pred.clone()\n",
    "        scale_boxes(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n",
    "\n",
    "        # Evaluate\n",
    "        if nl:\n",
    "            tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n",
    "            scale_boxes(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n",
    "            labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n",
    "            correct = process_batch(predn, labelsn, iouv)\n",
    "            if plots:\n",
    "                confusion_matrix.process_batch(predn, labelsn)\n",
    "        stats.append((correct, pred[:, 4], pred[:, 5], labels[:, 0]))  # (correct, conf, pcls, tcls)\n",
    "\n",
    "\n",
    "    # Plot images\n",
    "    if plots and batch_i < 3:\n",
    "        plot_images(im, teacher_labels, paths, save_dir / f\"val_batch{batch_i}_labels.jpg\", names)  # labels\n",
    "        plot_images(im, output_to_target(preds), paths, save_dir / f\"val_batch{batch_i}_pred.jpg\", names)  # pred\n",
    "        \n",
    "    model.cpu()\n",
    "\n",
    "# Compute metrics\n",
    "stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]  # to numpy\n",
    "if len(stats) and stats[0].any():\n",
    "    tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class\n",
    "\n",
    "# Print results\n",
    "pf = \"%22s\" + \"%11i\" * 2 + \"%11.3g\" * 4  # print format\n",
    "LOGGER.info(pf % (\"all\", seen, nt.sum(), mp, mr, map50, map))\n",
    "if nt.sum() == 0:\n",
    "    LOGGER.warning(f\"WARNING ⚠️ no labels found in {task} set, can not compute metrics without labels\")\n",
    "\n",
    "# Print results per class\n",
    "if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
    "    for i, c in enumerate(ap_class):\n",
    "        LOGGER.info(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "# Print speeds\n",
    "t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n",
    "if not training:\n",
    "    shape = (batch_size, 3, imgsz, imgsz)\n",
    "    LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}\" % t)\n",
    "\n",
    "# Plots\n",
    "if plots:\n",
    "    confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "\n",
    "# Return results\n",
    "if not training:\n",
    "    s = \"\"\n",
    "    LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
    "maps = np.zeros(nc) + map\n",
    "for i, c in enumerate(ap_class):\n",
    "    maps[c] = ap[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6239\n"
     ]
    }
   ],
   "source": [
    "print(len(stats[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5118911432068413, 0.1642187224292031, 0.17185789158503412, 0.10483564129379287, 0.005577079486101866, 0.024473417550325394, 0.003959154710173607) [    0.22006     0.11626     0.31403     0.14741     0.10484      0.1358     0.24865     0.13903     0.10484    0.098566           0   0.0099984           0      0.1089     0.10484     0.10484           0     0.10484     0.10484     0.10484     0.10484     0.10484     0.10484     0.10484    0.027928    0.077328\n",
      "    0.068934      0.1722     0.12057     0.10484     0.10484     0.10484           0     0.10484     0.10484     0.10484           0     0.10484           0     0.10484     0.10484     0.10484     0.10484     0.10484     0.10484     0.10484           0     0.10484     0.10484     0.10484     0.10484     0.10484\n",
      "     0.10484     0.10484     0.10484     0.10484     0.34566     0.10484    0.015462     0.10484      0.5686     0.10484     0.10484     0.10484     0.10484     0.10484           0     0.10484     0.10484     0.10484     0.10484     0.10484     0.10484           0           0     0.10484     0.10484     0.10484\n",
      "     0.10484     0.10484] (0.1283407211303711, 10.658784955739975, 0.8194006979465485)\n"
     ]
    }
   ],
   "source": [
    "print((mp, mr, map50, map, *(loss.cpu() / len(train_loader)).tolist()), maps, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
